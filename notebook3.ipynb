{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b17063",
   "metadata": {},
   "source": [
    "# üßë‚Äçüè´ Lab X ‚Äî Benchmarking Anthropic Models for Web-to-HTML Reconstruction\n",
    "\n",
    "**Goals**\n",
    "\n",
    "1. **Load** a screenshot of a webapp page (PNG/JPG)  \n",
    "2. **Invoke** multiple Claude models on AWS Bedrock  \n",
    "3. **Ask** each model to reproduce the HTML that generated the page  \n",
    "4. **Capture & compare** latency, token usage, reasoning chains, and cost  \n",
    "5. **Render** each generated HTML in the notebook for quick visual feedback\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab9bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Environment Setup\n",
    "\n",
    "# install AWS SDK, image handling, and table/output helpers\n",
    "%pip install -q boto3 pillow pandas tabulate rich tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1867b8c",
   "metadata": {},
   "source": [
    "# 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be48eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, json, logging, base64\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "import pandas as pd\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from tabulate import tabulate\n",
    "\n",
    "# For inline HTML rendering\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Logging & AWS Bedrock client ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"\n",
    ")\n",
    "log = logging.getLogger(\"webapp_html_benchmark\")\n",
    "console = Console()\n",
    "\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "bedrock_cfg = Config(connect_timeout=10, read_timeout=300, retries={\"max_attempts\":3})\n",
    "BEDROCK = boto3.client(\"bedrock-runtime\", region_name=AWS_REGION, config=bedrock_cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51027cbb",
   "metadata": {},
   "source": [
    "# 2. Model Catalogue & Cost Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3ec89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    key: str\n",
    "    display_name: str\n",
    "    model_id: str\n",
    "    max_tokens: int = 10_000\n",
    "    temperature: float = 0.0\n",
    "    thinking: Dict[str, Any] = field(default_factory=dict)\n",
    "    price_in_per_1M: float = 0.0\n",
    "    price_out_per_1M: float = 0.0\n",
    "\n",
    "    @property\n",
    "    def per_token_rates(self) -> tuple[float,float]:\n",
    "        return (self.price_in_per_1M/1e6, self.price_out_per_1M/1e6)\n",
    "\n",
    "# update with your own pricing\n",
    "PRICING = {\n",
    "    \"haiku3.5\":    (0.8, 4.0),\n",
    "    \"sonnet3.5v2\": (3.0,15.0),\n",
    "    \"sonnet3.7\":   (3.0,15.0),\n",
    "}\n",
    "\n",
    "# helper to prefix model IDs by region\n",
    "def _geo_prefix(region: str) -> str:\n",
    "    if region.startswith(\"us-\"): return \"us.\"\n",
    "    if region.startswith(\"eu-\"): return \"eu.\"\n",
    "    return \"ap.\"\n",
    "\n",
    "PFX = _geo_prefix(AWS_REGION)\n",
    "\n",
    "EVAL_MODELS: list[ModelConfig] = [\n",
    "    ModelConfig(\n",
    "        key=\"haiku3.5\",\n",
    "        display_name=\"Claude 3.5 Haiku\",\n",
    "        model_id=f\"{PFX}anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "        price_in_per_1M=PRICING[\"haiku3.5\"][0],\n",
    "        price_out_per_1M=PRICING[\"haiku3.5\"][1],\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        key=\"sonnet3.5v2\",\n",
    "        display_name=\"Claude 3.5 Sonnet v2\",\n",
    "        model_id=f\"{PFX}anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "        price_in_per_1M=PRICING[\"sonnet3.5v2\"][0],\n",
    "        price_out_per_1M=PRICING[\"sonnet3.5v2\"][1],\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        key=\"sonnet3.7_low\",\n",
    "        display_name=\"Claude 3.7 Sonnet (low reasoning)\",\n",
    "        model_id=f\"{PFX}anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        thinking={\"type\":\"enabled\",\"budget_tokens\":2048},\n",
    "        price_in_per_1M=PRICING[\"sonnet3.7\"][0],\n",
    "        price_out_per_1M=PRICING[\"sonnet3.7\"][1],\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        key=\"sonnet3.7_high\",\n",
    "        display_name=\"Claude 3.7 Sonnet (high reasoning)\",\n",
    "        model_id=f\"{PFX}anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        max_tokens=10000,\n",
    "        thinking={\"type\":\"enabled\",\"budget_tokens\":8192},\n",
    "        price_in_per_1M=PRICING[\"sonnet3.7\"][0],\n",
    "        price_out_per_1M=PRICING[\"sonnet3.7\"][1],\n",
    "    ),\n",
    "]\n",
    "\n",
    "def show_pricing():\n",
    "    tbl = Table(title=\"Price ‚Äî USD per million tokens\")\n",
    "    tbl.add_column(\"Model\")\n",
    "    tbl.add_column(\"Input $/M\", justify=\"right\")\n",
    "    tbl.add_column(\"Output $/M\", justify=\"right\")\n",
    "    for cfg in EVAL_MODELS:\n",
    "        in_rate, out_rate = cfg.price_in_per_1M, cfg.price_out_per_1M\n",
    "        tbl.add_row(cfg.display_name, f\"{in_rate:.2f}\", f\"{out_rate:.2f}\")\n",
    "    console.print(tbl)\n",
    "\n",
    "show_pricing()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f2ac1",
   "metadata": {},
   "source": [
    "# 3. Load & Encode Webapp Screenshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cc411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# path to your screenshot\n",
    "SCREENSHOT_PATH = \"webapp_screenshot.png\"\n",
    "\n",
    "# read & base64-encode\n",
    "with open(SCREENSHOT_PATH, \"rb\") as f:\n",
    "    raw_bytes = f.read()\n",
    "b64_image = base64.b64encode(raw_bytes).decode(\"utf-8\")\n",
    "\n",
    "# prompt template\n",
    "HTML_PROMPT = f\"\"\"\n",
    "Below is a screenshot of a web application, encoded in base64:\n",
    "{b64_image}\n",
    "Please reproduce the minimal, well-formatted HTML source (no CSS or JS) that,\n",
    "when rendered, would produce a visually equivalent page. Provide **only** the\n",
    "\\<html>‚Ä¶\\</html> code block‚Äîno explanations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2b7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_payload(prompt: str, cfg: ModelConfig) -> Dict[str,Any]:\n",
    "    body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": cfg.max_tokens,\n",
    "        \"temperature\": cfg.temperature,\n",
    "        \"messages\": [{\"role\":\"user\",\"content\": prompt}],\n",
    "    }\n",
    "    if cfg.thinking:\n",
    "        body[\"thinking\"] = cfg.thinking\n",
    "    return body\n",
    "\n",
    "def invoke_model(cfg: ModelConfig, prompt: str) -> Dict[str,Any]:\n",
    "    console.log(f\"‚Üí Invoking {cfg.display_name}\")\n",
    "    request = json.dumps(build_payload(prompt, cfg))\n",
    "    t0 = datetime.now()\n",
    "    try:\n",
    "        resp = BEDROCK.invoke_model(\n",
    "            modelId=cfg.model_id,\n",
    "            body=request,\n",
    "            contentType=\"application/json\",\n",
    "            accept=\"application/json\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return {\"name\":cfg.display_name, \"status\":\"ERROR\", \"error\":str(e)}\n",
    "    latency = (datetime.now() - t0).total_seconds()\n",
    "    data = json.loads(resp[\"body\"].read())\n",
    "\n",
    "    # extract model output & reasoning\n",
    "    full_text = \"\".join(chunk[\"text\"] for chunk in data[\"content\"] if chunk[\"type\"]==\"text\").strip()\n",
    "    reasoning  = \"\".join(chunk[\"thinking\"] for chunk in data[\"content\"] if chunk[\"type\"]==\"thinking\").strip()\n",
    "\n",
    "    tokens_out = len(full_text.split())\n",
    "    tokens_in  = len(prompt.split())\n",
    "    rate_in, rate_out = cfg.per_token_rates\n",
    "    cost_usd = round(tokens_in*rate_in + tokens_out*rate_out, 4)\n",
    "\n",
    "    sleep(1)  # pacing\n",
    "\n",
    "    return {\n",
    "        \"name\":           cfg.display_name,\n",
    "        \"status\":         \"OK\",\n",
    "        \"latency_s\":      round(latency,2),\n",
    "        \"tokens_in\":      tokens_in,\n",
    "        \"tokens_out\":     tokens_out,\n",
    "        \"thinking_tokens\": len(reasoning.split()),\n",
    "        \"cost_usd\":       cost_usd,\n",
    "        \"html\":           full_text,\n",
    "        \"reasoning\":      reasoning,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0b17c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run all models\n",
    "results = [invoke_model(cfg, HTML_PROMPT) for cfg in EVAL_MODELS]\n",
    "\n",
    "# show metrics table\n",
    "ok_results = [r for r in results if r[\"status\"]==\"OK\"]\n",
    "metrics_df = pd.DataFrame(ok_results)[\n",
    "    [\"name\",\"latency_s\",\"tokens_in\",\"tokens_out\",\"thinking_tokens\",\"cost_usd\"]\n",
    "]\n",
    "console.rule(\"üè∑  Run Metrics\")\n",
    "print(tabulate(metrics_df, headers=\"keys\", tablefmt=\"pretty\", showindex=False))\n",
    "\n",
    "# display each HTML and render it\n",
    "for res in ok_results:\n",
    "    console.rule(f\"üîß Output ‚Äî {res['name']}\")\n",
    "    # show raw HTML in a code block\n",
    "    print(\"```html\")\n",
    "    print(res[\"html\"])\n",
    "    print(\"```\")\n",
    "    # render inline\n",
    "    display(HTML(res[\"html\"]))\n",
    "    if res[\"reasoning\"]:\n",
    "        console.print(\"[i]Reasoning chain captured:[/i]\")\n",
    "        console.print(res[\"reasoning\"][:1000] + (\"‚Ä¶\" if len(res[\"reasoning\"])>1000 else \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb2ba63",
   "metadata": {},
   "source": [
    "# 6. Discussion & Next Steps\n",
    "Latency vs. Quality: ‚Ä¶\n",
    "\n",
    "Cost trade-offs: ‚Ä¶\n",
    "\n",
    "When to surface full reasoning: ‚Ä¶\n",
    "\n",
    "Tip: you can easily tweak temperature, max_tokens, or swap in other\n",
    "Bedrock-hosted models (e.g. Google Gemini via Vertex, OpenAI GPT 4, LLaMA on-prem)\n",
    "by adding another ModelConfig and rerunning the above cells.\n",
    "\n",
    "pgsql\n",
    "Copy\n",
    "Edit\n",
    "\n",
    "**Key points**  \n",
    "- We read and base64-encode the screenshot so it travels in the JSON payload.  \n",
    "- `invoke_model` collects everything: latency, in/out tokens, cost, and any reasoning.  \n",
    "- Rendering returned HTML is as simple as `display(HTML(generated_html))`.  \n",
    "\n",
    "Feel free to adapt this scaffold to more advanced ‚Äúcascades‚Äù (e.g. feeding the HTML back into a second mo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
