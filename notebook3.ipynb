{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b17063",
   "metadata": {},
   "source": [
    "# ğŸ§‘â€ğŸ« Lab X â€” Benchmarking Anthropic Models for Web-to-HTML Reconstruction\n",
    "\n",
    "**Goals**\n",
    "\n",
    "1. **Load** a screenshot of a webapp page (PNG/JPG)  \n",
    "2. **Invoke** multiple Claude models on AWS Bedrock  \n",
    "3. **Ask** each model to reproduce the HTML that generated the page  \n",
    "4. **Capture & compare** latency, token usage, reasoning chains, and cost  \n",
    "5. **Render** each generated HTML in the notebook for quick visual feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caab9bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 0. Environment Setup\n",
    "\n",
    "# install AWS SDK, image handling, and table/output helpers\n",
    "%pip install -q boto3 pillow pandas tabulate rich tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1867b8c",
   "metadata": {},
   "source": [
    "# 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be48eb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 17:49:28,200 [INFO] botocore.credentials: Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import logging\n",
    "import base64\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "import pandas as pd\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from tabulate import tabulate\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"webapp_html_benchmark\")\n",
    "console = Console()\n",
    "\n",
    "# AWS Bedrock client\n",
    "AWS_REGION = \"us-east-1\"  # or os.getenv(\"AWS_REGION\")\n",
    "bedrock_config = Config(connect_timeout=10, read_timeout=300, retries={\"max_attempts\": 3})\n",
    "bedrock_client = boto3.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=AWS_REGION,\n",
    "    config=bedrock_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51027cbb",
   "metadata": {},
   "source": [
    "# 2. Model Catalogue & Cost Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3ec89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                Price â€” USD per million tokens                 </span>\n",
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Model                              </span>â”ƒ<span style=\"font-weight: bold\"> Input $/M </span>â”ƒ<span style=\"font-weight: bold\"> Output $/M </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ Claude 3.5 Haiku                   â”‚      0.80 â”‚       4.00 â”‚\n",
       "â”‚ Claude 3.5 Sonnet v2               â”‚      3.00 â”‚      15.00 â”‚\n",
       "â”‚ Claude 3.7 Sonnet (low reasoning)  â”‚      3.00 â”‚      15.00 â”‚\n",
       "â”‚ Claude 3.7 Sonnet (high reasoning) â”‚      3.00 â”‚      15.00 â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                Price â€” USD per million tokens                 \u001b[0m\n",
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mModel                             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mInput $/M\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput $/M\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ Claude 3.5 Haiku                   â”‚      0.80 â”‚       4.00 â”‚\n",
       "â”‚ Claude 3.5 Sonnet v2               â”‚      3.00 â”‚      15.00 â”‚\n",
       "â”‚ Claude 3.7 Sonnet (low reasoning)  â”‚      3.00 â”‚      15.00 â”‚\n",
       "â”‚ Claude 3.7 Sonnet (high reasoning) â”‚      3.00 â”‚      15.00 â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    key: str\n",
    "    display_name: str\n",
    "    model_id: str\n",
    "    max_tokens: int = 10_000\n",
    "    temperature: float = 1.0\n",
    "    thinking: Dict[str, Any] = field(default_factory=dict)\n",
    "    price_in_per_1M: float = 0.0\n",
    "    price_out_per_1M: float = 0.0\n",
    "\n",
    "    @property\n",
    "    def rate_in(self) -> float:\n",
    "        return self.price_in_per_1M / 1e6\n",
    "\n",
    "    @property\n",
    "    def rate_out(self) -> float:\n",
    "        return self.price_out_per_1M / 1e6\n",
    "\n",
    "# Update with your regionâ€prefix logic if needed\n",
    "def _geo_prefix(region: str) -> str:\n",
    "    if region.startswith(\"us-\"):\n",
    "        return \"us.\"\n",
    "    if region.startswith(\"eu-\"):\n",
    "        return \"eu.\"\n",
    "    return \"ap.\"\n",
    "\n",
    "PFX = _geo_prefix(AWS_REGION)\n",
    "\n",
    "# Perâ€millionâ€token pricing\n",
    "PRICING = {\n",
    "    \"haiku3.5\":    (0.8, 4.0),\n",
    "    \"sonnet3.5v2\": (3.0,15.0),\n",
    "    \"sonnet3.7\":   (3.0,15.0),\n",
    "}\n",
    "\n",
    "EVAL_MODELS: List[ModelConfig] = [\n",
    "    ModelConfig(\n",
    "        key=\"haiku3.5\",\n",
    "        display_name=\"Claude 3.5 Haiku\",\n",
    "        model_id=f\"{PFX}anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "        price_in_per_1M=PRICING[\"haiku3.5\"][0],\n",
    "        price_out_per_1M=PRICING[\"haiku3.5\"][1],\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        key=\"sonnet3.5v2\",\n",
    "        display_name=\"Claude 3.5 Sonnet v2\",\n",
    "        model_id=f\"{PFX}anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "        price_in_per_1M=PRICING[\"sonnet3.5v2\"][0],\n",
    "        price_out_per_1M=PRICING[\"sonnet3.5v2\"][1],\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        key=\"sonnet3.7_low\",\n",
    "        display_name=\"Claude 3.7 Sonnet (low reasoning)\",\n",
    "        model_id=f\"{PFX}anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        thinking={\"type\": \"enabled\", \"budget_tokens\": 2_048},\n",
    "        price_in_per_1M=PRICING[\"sonnet3.7\"][0],\n",
    "        price_out_per_1M=PRICING[\"sonnet3.7\"][1],\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        key=\"sonnet3.7_high\",\n",
    "        display_name=\"Claude 3.7 Sonnet (high reasoning)\",\n",
    "        model_id=f\"{PFX}anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        max_tokens=10_000,\n",
    "        thinking={\"type\": \"enabled\", \"budget_tokens\": 8_192},\n",
    "        price_in_per_1M=PRICING[\"sonnet3.7\"][0],\n",
    "        price_out_per_1M=PRICING[\"sonnet3.7\"][1],\n",
    "    ),\n",
    "]\n",
    "\n",
    "def show_pricing_table(models: List[ModelConfig]) -> None:\n",
    "    table = Table(title=\"Price â€” USD per million tokens\")\n",
    "    table.add_column(\"Model\", no_wrap=True)\n",
    "    table.add_column(\"Input $/M\", justify=\"right\")\n",
    "    table.add_column(\"Output $/M\", justify=\"right\")\n",
    "    for m in models:\n",
    "        table.add_row(m.display_name, f\"{m.price_in_per_1M:.2f}\", f\"{m.price_out_per_1M:.2f}\")\n",
    "    console.print(table)\n",
    "\n",
    "# Display pricing\n",
    "show_pricing_table(EVAL_MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f2ac1",
   "metadata": {},
   "source": [
    "# 3. Load & Encode Webapp Screenshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3cc411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def encode_image_to_base64(\n",
    "    image_path: Path,\n",
    "    max_dim: int = 512,\n",
    "    jpeg_quality: int = 30\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Opens an image, downscales it so its longest side is max_dim,\n",
    "    saves it as a JPEG with the given quality into memory,\n",
    "    and returns the base64 string.\n",
    "    \"\"\"\n",
    "    if not image_path.exists():\n",
    "        logger.error(f\"Screenshot not found: {image_path}\")\n",
    "        raise FileNotFoundError(f\"{image_path} does not exist\")\n",
    "\n",
    "    # 1) Open & downscale\n",
    "    img = Image.open(image_path)\n",
    "    img.thumbnail((max_dim, max_dim), Image.LANCZOS)\n",
    "\n",
    "    # 2) JPEG-compress into a buffer\n",
    "    buffer = BytesIO()\n",
    "    img = img.convert(\"RGB\")  # JPEG requires no alpha\n",
    "    img.save(buffer, format=\"JPEG\", quality=jpeg_quality, optimize=True)\n",
    "    buffer.seek(0)\n",
    "\n",
    "    # 3) Base64 encode\n",
    "    b64 = base64.b64encode(buffer.read()).decode(\"utf-8\")\n",
    "    return b64\n",
    "\n",
    "# Usage:\n",
    "SCREENSHOT_PATH = Path(\"data\") / \"nextflix.png\"\n",
    "encoded_image = encode_image_to_base64(\n",
    "    SCREENSHOT_PATH,\n",
    "    max_dim=512,        # scale longest side to â‰¤512px\n",
    "    jpeg_quality=30     # tune between 1â€“100 for size vs. fidelity\n",
    ")\n",
    "\n",
    "HTML_PROMPT = f\"\"\"\n",
    "Below is a downscaled, JPEG-compressed screenshot of a web application, encoded in base64:\n",
    "{encoded_image}\n",
    "\n",
    "Please reproduce the minimal, well-formatted HTML source (CSS inline, no JS)\n",
    "that, when rendered, would produce a visually equivalent page.\n",
    "Provide **only** the <html>â€¦</html> code blockâ€”no explanations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba696b4",
   "metadata": {},
   "source": [
    "# 4. Invocation & Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1d2b7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "def build_payload(prompt: str, cfg: ModelConfig) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": cfg.max_tokens,\n",
    "        \"temperature\": cfg.temperature,\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        **({\"thinking\": cfg.thinking} if cfg.thinking else {}),\n",
    "    }\n",
    "\n",
    "def parse_bedrock_response(\n",
    "    body_bytes: bytes,\n",
    "    prompt: str,\n",
    "    cfg: ModelConfig\n",
    ") -> Tuple[str, str, int, int]:\n",
    "    \"\"\"\n",
    "    Returns (full_text, reasoning, tokens_in, tokens_out).\n",
    "    Attempts to use reported usage if present, else falls back to splitting on whitespace.\n",
    "    \"\"\"\n",
    "    data = json.loads(body_bytes)\n",
    "\n",
    "    # 1) Try to pull 'usage' fields if available\n",
    "    usage = data.get(\"usage\", {})\n",
    "    tokens_in  = usage.get(\"prompt_tokens\", 0) or usage.get(\"promptTokens\", 0) or 0\n",
    "    tokens_out = usage.get(\"completion_tokens\", 0) or usage.get(\"completionTokens\", 0) or 0\n",
    "\n",
    "    # 2) Extract text + reasoning\n",
    "    content = data.get(\"content\") or data.get(\"completions\") or []\n",
    "    texts, thoughts = [], []\n",
    "    if isinstance(content, list):\n",
    "        for chunk in content:\n",
    "            if chunk.get(\"type\") == \"text\":\n",
    "                texts.append(chunk.get(\"text\",\"\"))\n",
    "            if \"thinking\" in chunk:\n",
    "                thoughts.append(chunk[\"thinking\"])\n",
    "    else:\n",
    "        texts = [ data.get(\"completion\",\"\") ]\n",
    "    full_text = \"\".join(texts).strip()\n",
    "    reasoning = \"\".join(thoughts).strip()\n",
    "\n",
    "    # 3) Fallback to whitespaceâ€split counts if usage wasn't provided\n",
    "    if tokens_in == 0:\n",
    "        tokens_in = len(prompt.split())\n",
    "    if tokens_out == 0:\n",
    "        tokens_out = len(full_text.split())\n",
    "\n",
    "    return full_text, reasoning, tokens_in, tokens_out\n",
    "\n",
    "def invoke_model(cfg: ModelConfig, prompt: str) -> Dict[str, Any]:\n",
    "    logger.info(f\"Invoking model: {cfg.display_name}\")\n",
    "    payload = build_payload(prompt, cfg)\n",
    "    start = datetime.now(timezone.utc)\n",
    "\n",
    "    try:\n",
    "        resp = bedrock_client.invoke_model(\n",
    "            modelId=cfg.model_id,\n",
    "            contentType=\"application/json\",\n",
    "            accept=\"application/json\",\n",
    "            body=json.dumps(payload).encode(\"utf-8\"),\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        logger.error(f\"Error from {cfg.display_name}: {exc}\")\n",
    "        return {\"name\": cfg.display_name, \"status\": \"ERROR\", \"error\": str(exc)}\n",
    "\n",
    "    latency = (datetime.now(timezone.utc) - start).total_seconds()\n",
    "    body_bytes = resp[\"body\"].read()\n",
    "\n",
    "    # â† pass prompt along so fallback works\n",
    "    html, reasoning, tokens_in, tokens_out = parse_bedrock_response(body_bytes, prompt, cfg)\n",
    "\n",
    "    cost = round(tokens_in * cfg.rate_in + tokens_out * cfg.rate_out, 4)\n",
    "    sleep(1)  # pacing between calls\n",
    "\n",
    "    return {\n",
    "        \"name\":            cfg.display_name,\n",
    "        \"status\":          \"OK\",\n",
    "        \"latency_s\":       round(latency, 2),\n",
    "        \"tokens_in\":       tokens_in,\n",
    "        \"tokens_out\":      tokens_out,\n",
    "        \"thinking_tokens\": len(reasoning.split()),\n",
    "        \"cost_usd\":        cost,\n",
    "        \"html\":            html,\n",
    "        \"reasoning\":       reasoning,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de943212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which model do you want to benchmark today?  \n",
    "# Change this to any key in EVAL_MODELS: \"haiku3.5\", \"sonnet3.5v2\", \"sonnet3.7_low\", \"sonnet3.7_high\"\n",
    "SELECT_MODEL = \"sonnet3.5v2\"\n",
    "\n",
    "# Find the matching ModelConfig (will error if you typo)\n",
    "selected_cfgs = [m for m in EVAL_MODELS if m.key == SELECT_MODEL]\n",
    "if not selected_cfgs:\n",
    "    raise ValueError(f\"No model matching key={SELECT_MODEL!r}. Valid keys: {[m.key for m in EVAL_MODELS]}\")\n",
    "# Optionally: you could support SELECT_MODEL = None to run all again\n",
    "#    if SELECT_MODEL is None: selected_cfgs = EVAL_MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a0b17c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 17:49:28,668 [INFO] webapp_html_benchmark: Invoking model: Claude 3.5 Sonnet v2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span>ğŸ· Run Metrics â€” sonnet3.5v2<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \u001b[0mğŸ· Run Metrics â€” sonnet3.5v2\u001b[92m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------+-----------+------------+-----------------+----------+\n",
      "|         name         | latency_s | tokens_in | tokens_out | thinking_tokens | cost_usd |\n",
      "+----------------------+-----------+-----------+------------+-----------------+----------+\n",
      "| Claude 3.5 Sonnet v2 |   9.46    |    41     |    109     |        0        |  0.0018  |\n",
      "+----------------------+-----------+-----------+------------+-----------------+----------+\n"
     ]
    }
   ],
   "source": [
    "# After:\n",
    "results = []\n",
    "for cfg in selected_cfgs:\n",
    "    results.append(invoke_model(cfg, HTML_PROMPT))\n",
    "\n",
    "# split successes / failures exactly as before...\n",
    "successes = [r for r in results if r[\"status\"] == \"OK\"]\n",
    "failures  = [r for r in results if r[\"status\"] != \"OK\"]\n",
    "\n",
    "if failures:\n",
    "    console.print(\"[yellow]âš ï¸ Model invocation failed:[/yellow]\")\n",
    "    console.print(f\" â€¢ [bold]{failures[0]['name']}[/bold]: {failures[0]['error']}\")\n",
    "\n",
    "if successes:\n",
    "    df = pd.DataFrame(successes)[\n",
    "        [\"name\",\"latency_s\",\"tokens_in\",\"tokens_out\",\"thinking_tokens\",\"cost_usd\"]\n",
    "    ]\n",
    "    console.rule(f\"ğŸ· Run Metrics â€” {SELECT_MODEL}\")\n",
    "    print(tabulate(df, headers=\"keys\", tablefmt=\"pretty\", showindex=False))\n",
    "else:\n",
    "    console.print(\"[bold red]âŒ No successful runs.[/bold red]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5a2680",
   "metadata": {},
   "source": [
    "# 6. Render Outputs Inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf279cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span>ğŸ”§ Output â€” Claude <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5</span> Sonnet v2<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \u001b[0mğŸ”§ Output â€” Claude \u001b[1;36m3.5\u001b[0m Sonnet v2\u001b[92m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style='white-space: pre-wrap; border:1px solid #ddd; padding:10px;'><html>\n",
       "<head>\n",
       "<title>Web Application</title>\n",
       "</head>\n",
       "<body style=\"margin: 0; padding: 0; font-family: Arial, sans-serif; background: #f4f4f4;\">\n",
       "  <div style=\"width: 100%; min-height: 50px; background: #383838; color: white; display: flex; align-items: center; padding: 0 20px;\">\n",
       "    <div style=\"flex: 1;\">\n",
       "      <h2 style=\"margin: 0;\">Logo</h2>\n",
       "    </div>\n",
       "    <div style=\"display: flex; gap: 20px;\">\n",
       "      <div style=\"padding: 10px;\">Home</div>\n",
       "      <div style=\"padding: 10px;\">About</div>\n",
       "      <div style=\"padding: 10px;\">Contact</div>\n",
       "    </div>\n",
       "  </div>\n",
       "  <div style=\"max-width: 1200px; margin: 20px auto; padding: 20px; background: white; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\">\n",
       "    <h1 style=\"color: #333; margin-bottom: 20px;\">Welcome to Our Application</h1>\n",
       "    <p style=\"color: #666; line-height: 1.6;\">\n",
       "      This is the main content area of our web application. Here you can find important information and updates about our services.\n",
       "    </p>\n",
       "  </div>\n",
       "</body>\n",
       "</html></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head>\n",
       "<title>Web Application</title>\n",
       "</head>\n",
       "<body style=\"margin: 0; padding: 0; font-family: Arial, sans-serif; background: #f4f4f4;\">\n",
       "  <div style=\"width: 100%; min-height: 50px; background: #383838; color: white; display: flex; align-items: center; padding: 0 20px;\">\n",
       "    <div style=\"flex: 1;\">\n",
       "      <h2 style=\"margin: 0;\">Logo</h2>\n",
       "    </div>\n",
       "    <div style=\"display: flex; gap: 20px;\">\n",
       "      <div style=\"padding: 10px;\">Home</div>\n",
       "      <div style=\"padding: 10px;\">About</div>\n",
       "      <div style=\"padding: 10px;\">Contact</div>\n",
       "    </div>\n",
       "  </div>\n",
       "  <div style=\"max-width: 1200px; margin: 20px auto; padding: 20px; background: white; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\">\n",
       "    <h1 style=\"color: #333; margin-bottom: 20px;\">Welcome to Our Application</h1>\n",
       "    <p style=\"color: #666; line-height: 1.6;\">\n",
       "      This is the main content area of our web application. Here you can find important information and updates about our services.\n",
       "    </p>\n",
       "  </div>\n",
       "</body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for res in successes:\n",
    "    console.rule(f\"ğŸ”§ Output â€” {res['name']}\")\n",
    "    # raw HTML\n",
    "    display(HTML(f\"<pre style='white-space: pre-wrap; border:1px solid #ddd; padding:10px;'>\"\n",
    "                 f\"{res['html']}</pre>\"))\n",
    "    # visual render\n",
    "    display(HTML(res[\"html\"]))\n",
    "    if res[\"reasoning\"]:\n",
    "        console.print(\"[italic]Captured reasoning (truncated):[/italic]\")\n",
    "        console.print(res[\"reasoning\"][:500] + (\"â€¦\" if len(res[\"reasoning\"]) > 500 else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb2ba63",
   "metadata": {},
   "source": [
    "# 7. Discussion & Next Steps\n",
    "Latency vs. Quality: â€¦\n",
    "\n",
    "Cost trade-offs: â€¦\n",
    "\n",
    "When to surface full reasoning: â€¦\n",
    "\n",
    "Tip: you can easily tweak temperature, max_tokens, or swap in other\n",
    "Bedrock-hosted models (e.g. Google Gemini via Vertex, OpenAI GPT 4, LLaMA on-prem)\n",
    "by adding another ModelConfig and rerunning the above cells.\n",
    "\n",
    "pgsql\n",
    "Copy\n",
    "Edit\n",
    "\n",
    "**Key points**  \n",
    "- We read and base64-encode the screenshot so it travels in the JSON payload.  \n",
    "- `invoke_model` collects everything: latency, in/out tokens, cost, and any reasoning.  \n",
    "- Rendering returned HTML is as simple as `display(HTML(generated_html))`.  \n",
    "\n",
    "Feel free to adapt this scaffold to more advanced â€œcascadesâ€ (e.g. feeding the HTML back into a second mo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
