{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧑‍🏫 Lab 2 — Benchmarking Anthropic Models for Coding Tasks\n",
    "\n",
    "**Goals**\n",
    "\n",
    "1. **Load** a random coding question from the *cais/hle* benchmark  \n",
    "2. **Invoke** 5 Anthropic models hosted on Bedrock (up to Claude 3.7 Sonnet)  \n",
    "3. **Capture & compare** latency, token usage, *reasoning chains*, and cost  \n",
    "4. Discuss why larger reasoning budgets help—and what they cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Benchmarks\n",
    "\n",
    "Benchmarks are standardized tests designed to evaluate and compare the performance of machine learning models on specific tasks. While benchmarks provide valuable insights into model capabilities, they have limitations:\n",
    "\n",
    "Not always reflective of real-world performance: Benchmarks can be optimized or \"overfit\" by models, leading to high scores that don't necessarily translate to practical scenarios.\n",
    "\n",
    "Limited scope: Benchmarks typically measure only specific aspects of performance, potentially overlooking other crucial model attributes such as robustness, interpretability, and adaptability.\n",
    "\n",
    "Static and dated: Many benchmarks are fixed datasets, making them vulnerable to becoming outdated as model capabilities and real-world demands evolve rapidly.\n",
    "\n",
    "Therefore, while benchmarks offer a useful baseline, they should not be the sole criterion for model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Dive: Humanity Last Exam (HLE)\n",
    "\n",
    "The Humanity Last Exam (HLE) is a benchmark specifically designed to evaluate coding and reasoning capabilities of advanced language models:\n",
    "\n",
    "Purpose: Assess models' ability to reason logically, write correct code, and understand complex instructions.\n",
    "\n",
    "Format: Comprises various programming problems with varying levels of difficulty, each accompanied by known correct solutions for comparison.\n",
    "\n",
    "Metrics: Models are evaluated based on accuracy, reasoning quality, token efficiency, latency, and overall solution correctness.\n",
    "\n",
    "By using the HLE, researchers and practitioners can better understand models' reasoning skills and practical utility for programming tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment Setup\n",
    "\n",
    "Install required Python packages for AWS Bedrock, data loading, and rich output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q boto3 langchain_aws datasets pandas tabulate tqdm rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we set up logging, AWS connectivity, and a helper dataclass that\n",
    "holds both **input** and **output** token prices plus an optional\n",
    "`thinking` stanza (only used by Claude 3.7).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json, logging, os\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from botocore.config import Config\n",
    "import boto3, pandas as pd\n",
    "from datasets import load_dataset\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from tabulate import tabulate\n",
    "from time import sleep\n",
    "\n",
    "# logging & AWS\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\")\n",
    "log, console = logging.getLogger(\"bedrock_benchmark\"), Console()\n",
    "\n",
    "REGION = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "\n",
    "client_cfg = Config(\n",
    "    connect_timeout=10,\n",
    "    read_timeout=300,    # 5 minutes\n",
    "    retries={'max_attempts': 3}\n",
    ")\n",
    "BEDROCK = boto3.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=REGION,\n",
    "    config=client_cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Dataclass ----------------------------------------------------------------\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    key: str\n",
    "    name: str\n",
    "    model_id: str\n",
    "    max_tokens: int = 10_000\n",
    "    temperature: float = 1.0\n",
    "    thinking: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    # pricing (USD per million tokens)\n",
    "    price_in_per_1M: float = 0.0\n",
    "    price_out_per_1M: float = 0.0\n",
    "\n",
    "    # helpers\n",
    "    @property\n",
    "    def rates(self) -> tuple[float, float]:\n",
    "        \"\"\"Return $ cost *per single token* (input, output).\"\"\"\n",
    "        return (self.price_in_per_1M / 1_000_000, self.price_out_per_1M / 1_000_000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Catalogue & Cost Reference\n",
    "We define our six evaluation configurations, along with approximate cost per 1M tokens.\n",
    "\n",
    "The table below lists the models we’ll test and their **input / output**\n",
    "token prices (USD per million).  \n",
    "*Note:* Opus and Sonnet 3.5 require explicit enablement in the Bedrock\n",
    "console—if you don’t have access yet they will appear as **NO_ACCESS**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Pricing table (Apr-2025)  ──  (input $, output $) per million tokens\n",
    "PRICING_PER_1M = {\n",
    "    \"haiku3.5\":     (0.8,  4.0),\n",
    "    \"opus\":         (15.0, 75.0),\n",
    "    \"sonnet3.5v2\":  (3.0, 15.0),\n",
    "    \"sonnet3.7\":    (3.0, 15.0),\n",
    "}\n",
    "\n",
    "def geo_prefix(region: str) -> str:\n",
    "    return \"us.\" if region.startswith(\"us-\") else \"eu.\" if region.startswith(\"eu-\") else \"ap.\"\n",
    "\n",
    "PFX = geo_prefix(REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Models under test --------------------------------------------------------\n",
    "EVAL_MODELS: list[ModelConfig] = [\n",
    "    ModelConfig(\n",
    "        \"haiku3.5\", \"Claude 3.5 Haiku\",\n",
    "        f\"{PFX}anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "        price_in_per_1M=PRICING_PER_1M[\"haiku3.5\"][0],\n",
    "        price_out_per_1M=PRICING_PER_1M[\"haiku3.5\"][1],\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        \"opus\", \"Claude 3 Opus\",\n",
    "        f\"{PFX}anthropic.claude-3-opus-20240229-v1:0\",\n",
    "        price_in_per_1M=PRICING_PER_1M[\"opus\"][0],\n",
    "        price_out_per_1M=PRICING_PER_1M[\"opus\"][1],\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        \"sonnet3.5v2\", \"Claude 3.5 Sonnet v2\",\n",
    "        f\"{PFX}anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "        price_in_per_1M=PRICING_PER_1M[\"sonnet3.5v2\"][0],\n",
    "        price_out_per_1M=PRICING_PER_1M[\"sonnet3.5v2\"][1],\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        \"sonnet3.7_low\", \"Claude 3.7 Sonnet (low reasoning)\",\n",
    "        f\"{PFX}anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        thinking={\"type\": \"enabled\", \"budget_tokens\": 2048},\n",
    "        price_in_per_1M=PRICING_PER_1M[\"sonnet3.7\"][0],\n",
    "        price_out_per_1M=PRICING_PER_1M[\"sonnet3.7\"][1],\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        \"sonnet3.7_high\", \"Claude 3.7 Sonnet (high reasoning)\",\n",
    "        f\"{PFX}anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        max_tokens=10_000,\n",
    "        thinking={\"type\": \"enabled\", \"budget_tokens\": 8192},\n",
    "        price_in_per_1M=PRICING_PER_1M[\"sonnet3.7\"][0],\n",
    "        price_out_per_1M=PRICING_PER_1M[\"sonnet3.7\"][1],\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Helper: pretty pricing table --------------------------------------------\n",
    "def show_costs() -> None:\n",
    "    tbl = Table(title=\"Price — $ per million tokens\")\n",
    "    tbl.add_column(\"key\")\n",
    "    tbl.add_column(\"input $/M\", justify=\"right\")\n",
    "    tbl.add_column(\"output $/M\", justify=\"right\")\n",
    "    for m in EVAL_MODELS:\n",
    "        tbl.add_row(m.key, f\"{m.price_in_per_1M:.2f}\", f\"{m.price_out_per_1M:.2f}\")\n",
    "    console.print(tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_costs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load a Coding Prompt\n",
    "We grab one random test item from *cais/hle*.  \n",
    "Feel free to re-run the cell to sample a different coding problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Dataset prompt loader ───────────────────────────────────────────────\n",
    "from functools import lru_cache\n",
    "from datasets import load_dataset\n",
    "from random import Random\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _hle_dataset() -> \"datasets.Dataset\":\n",
    "    \"\"\"Cache the dataset so multiple calls are instant.\"\"\"\n",
    "    console.print(\"[grey]Loading *cais/hle* … (cached on first call)\")\n",
    "    return load_dataset(\"cais/hle\", split=\"test\", cache_dir=\"./hf_cache\")\n",
    "\n",
    "def load_prompt(\n",
    "    *,\n",
    "    question_id: str | None = None,\n",
    "    row_idx: int | None = None,\n",
    "    seed: int | None = None,\n",
    ") -> tuple[str, str, dict]:\n",
    "    \"\"\"\n",
    "    Fetch a coding question–answer pair from the HLE benchmark.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question_id : str, optional\n",
    "        The exact HLE `id` field (e.g. \"hle_py_00846\").\n",
    "    row_idx : int, optional\n",
    "        Direct row index into the test split (0-based).\n",
    "    seed : int, optional\n",
    "        If neither `question_id` nor `row_idx` is passed, a random row\n",
    "        is chosen using this seed (defaults to system RNG).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    question : str\n",
    "    answer   : str\n",
    "    meta     : dict   # keys: id, difficulty, prompt_len, answer_len\n",
    "    \"\"\"\n",
    "    ds = _hle_dataset()\n",
    "\n",
    "    # ❶ Resolve the row\n",
    "    if question_id is not None:\n",
    "        try:\n",
    "            row = ds.filter(lambda r: r[\"id\"] == question_id)[0]\n",
    "        except IndexError:\n",
    "            raise ValueError(f\"HLE id “{question_id}” not found\") from None\n",
    "\n",
    "    elif row_idx is not None:\n",
    "        if not (0 <= row_idx < len(ds)):\n",
    "            raise IndexError(f\"row_idx must be in [0, {len(ds)-1}]\")\n",
    "        row = ds[int(row_idx)]\n",
    "\n",
    "    else:  # random\n",
    "        rng = Random(seed)\n",
    "        row = rng.choice(ds)\n",
    "\n",
    "    # ❷ Pretty print once per call\n",
    "    console.rule(f\"📝  Selected question  —  {row['id']}\")\n",
    "    console.print(row[\"question\"])\n",
    "    console.print(f\"📝  Correct answer: {row['answer']}\")\n",
    "\n",
    "    # ❸ Meta for later analytics\n",
    "    meta = dict(\n",
    "        id=row[\"id\"],\n",
    "        difficulty=row.get(\"difficulty\", \"N/A\"),\n",
    "        prompt_len=len(row[\"question\"].split()),\n",
    "        answer_len=len(row[\"answer\"].split()),\n",
    "    )\n",
    "\n",
    "    return row[\"question\"], row[\"answer\"], meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- random question (same behaviour as before)\n",
    "# prompt, reference_answer, meta = load_prompt()\n",
    "\n",
    "# # --- deterministic random (useful in slides / demos)\n",
    "# prompt, reference_answer, meta = load_prompt(seed=42)\n",
    "\n",
    "# # --- fetch a specific HLE task by its unique id\n",
    "# prompt, reference_answer, meta = load_prompt(question_id=\"hle_py_00846\")\n",
    "\n",
    "# # --- or simply by row index\n",
    "prompt, reference_answer, meta = load_prompt(row_idx=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Payload & Invoke\n",
    "`build_payload` constructs the Bedrock-compatible request.  \n",
    "`invoke` sends it **and captures reasoning chains** when available\n",
    "(type `\"thinking\"` content blocks returned by Claude 3.7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_payload(prompt: str, cfg: ModelConfig) -> Dict[str, Any]:\n",
    "    pl = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": cfg.max_tokens,\n",
    "        \"temperature\": cfg.temperature,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "    }\n",
    "    if cfg.thinking:\n",
    "        pl[\"thinking\"] = cfg.thinking\n",
    "    return pl\n",
    "\n",
    "def invoke(cfg: ModelConfig, prompt: str) -> Dict[str, Any]:\n",
    "    console.log(f\"Invoking {cfg.name} …\")\n",
    "    try:\n",
    "        t0 = datetime.now()\n",
    "        resp = BEDROCK.invoke_model(\n",
    "            body=json.dumps(build_payload(prompt, cfg)),\n",
    "            modelId=cfg.model_id,\n",
    "            contentType=\"application/json\",\n",
    "            accept=\"application/json\",\n",
    "        )\n",
    "        latency = (datetime.now() - t0).total_seconds()\n",
    "    except BEDROCK.exceptions.AccessDeniedException:\n",
    "        return {\"name\": cfg.name, \"status\": \"NO_ACCESS\"}\n",
    "    except Exception as e:\n",
    "        log.error(\"%s failed: %s\", cfg.name, e)\n",
    "        return {\"name\": cfg.name, \"status\": \"ERROR\", \"error\": str(e)}\n",
    "\n",
    "    data = json.loads(resp[\"body\"].read())\n",
    "\n",
    "    # extract answer & reasoning\n",
    "    txt_out = \"\\n\".join(c[\"text\"]  for c in data[\"content\"] if c[\"type\"] == \"text\").strip()\n",
    "    thinking_txt = \"\\n\".join(c[\"thinking\"] for c in data[\"content\"] if c[\"type\"] == \"thinking\").strip()\n",
    "    thinking_tokens = len(thinking_txt.split())\n",
    "\n",
    "    tokens_out = len(txt_out.split())\n",
    "    tokens_in  = len(prompt.split())\n",
    "\n",
    "    rate_in, rate_out = cfg.rates\n",
    "    usd_cost = round(tokens_in * rate_in + tokens_out * rate_out, 4)\n",
    "\n",
    "    sleep(1)\n",
    "    # ← price fields included so later cells don’t need cfg look-ups\n",
    "    return {\n",
    "        \"name\": cfg.name,\n",
    "        \"status\": \"OK\",\n",
    "        \"latency_s\": round(latency, 2),\n",
    "        \"tokens_in\": tokens_in,\n",
    "        \"tokens_out\": tokens_out,\n",
    "        \"thinking_tokens\": thinking_tokens,\n",
    "        \"approx_cost_usd\": usd_cost,\n",
    "        \"answer\": txt_out or \"[empty]\",\n",
    "        \"thinking\": thinking_txt,\n",
    "        \"price_in_per_1M\": cfg.price_in_per_1M,\n",
    "        \"price_out_per_1M\": cfg.price_out_per_1M,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Benchmarks & Review\n",
    "The cell below runs every model, prints a metric table, and then shows\n",
    "each answer.  \n",
    "For Claude 3.7 entries we also reveal the **internal reasoning chain**\n",
    "(truncated to the first 1 000 chars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise(results: List[Dict[str, Any]]) -> None:\n",
    "    ok = [r for r in results if r[\"status\"] == \"OK\"]\n",
    "    if ok:\n",
    "        cols = [\"name\", \"latency_s\", \"tokens_in\", \"tokens_out\",\n",
    "                \"thinking_tokens\", \"approx_cost_usd\"]\n",
    "        df = pd.DataFrame(ok)[cols]\n",
    "        console.rule(\"Run metrics\")\n",
    "        print(tabulate(df, headers=\"keys\", tablefmt=\"pretty\", showindex=False))\n",
    "\n",
    "    for r in results:\n",
    "        console.rule(f\"Answer — {r['name']} [{r['status']}]\")\n",
    "        console.print(r.get(\"answer\", r.get(\"error\", \"\")))\n",
    "        if r.get(\"thinking\"):\n",
    "            console.print(\"\\n[i]Full reasoning chain:[/i]\")\n",
    "            console.print(r[\"thinking\"])        # ← NO TRUNCATION\n",
    "\n",
    "\n",
    "console.rule(\"Running benchmarks\")\n",
    "results = [invoke(cfg, prompt) for cfg in EVAL_MODELS]\n",
    "summarise(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to fetch a model row safely\n",
    "def pick(prefix: str):\n",
    "    return next((r for r in results if r[\"name\"].lower().startswith(prefix.lower())), None)\n",
    "\n",
    "def fmt(row: dict | None, key: str, spec: str = \"{}\"):\n",
    "    \"\"\"Safe formatter: dash if row/key missing.\"\"\"\n",
    "    return \"—\" if row is None or key not in row else spec.format(row[key])\n",
    "\n",
    "haiku  = pick(\"claude 3.5 haiku\")\n",
    "opus   = pick(\"claude 3 opus\")\n",
    "lo37   = pick(\"claude 3.7 sonnet (low\")\n",
    "hi37   = pick(\"claude 3.7 sonnet (high\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "md = f\"\"\"\n",
    "## 🔎 Latency · Cost · Quality snapshot ({datetime.now():%d %b %Y})\n",
    "\n",
    "| Model | Latency (s) | Out-tokens | Reasoning tokens | Cost (USD) | Notes |\n",
    "|-------|------------:|-----------:|-----------------:|-----------:|-------|\n",
    "| **Claude 3.5 Haiku**          | {fmt(haiku,'latency_s','{:.2f}')} | {fmt(haiku,'tokens_out')} | —   | {fmt(haiku,'approx_cost_usd','{:.4f}')} | Fast & cheap |\n",
    "| **Claude 3 Opus**             | {fmt(opus,'latency_s','{:.2f}')}  | {fmt(opus,'tokens_out')}  | —   | {fmt(opus,'approx_cost_usd','{:.4f}')}  | Premium accuracy |\n",
    "| **Claude 3.7 Sonnet (low)**   | {fmt(lo37,'latency_s','{:.2f}')}  | {fmt(lo37,'tokens_out')}  | {fmt(lo37,'thinking_tokens')} | {fmt(lo37,'approx_cost_usd','{:.4f}')} | Concise reasoning |\n",
    "| **Claude 3.7 Sonnet (high)**  | {fmt(hi37,'latency_s','{:.2f}')}  | {fmt(hi37,'tokens_out')}  | {fmt(hi37,'thinking_tokens')} | {fmt(hi37,'approx_cost_usd','{:.4f}')} | Full rationale |\n",
    "\"\"\"\n",
    "display(Markdown(md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Interpretation*\n",
    "\n",
    "* Haiku 3.5 gives you a usable answer in ~4 s for < 1 cents.\n",
    "* Low-budget Sonnet 3.7 surfaces its reasoning (≈ 2 k tokens) with only a 70 ms/token penalty.\n",
    "* The high-budget preset is valuable when you **must** audit every step or need a > 8 k context window; otherwise the cost-latency trade-off is hard to justify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 2 · What Are We Paying for with `thinking`?\n",
    "\n",
    "| Sonnet 3.7 preset | Reasoning tokens | Δ $ vs Haiku | Where it helped |\n",
    "|-------------------|-----------------:|-------------:|-----------------|\n",
    "| **low (2 048)**  | 2 048            | **+ 0.0715** | clarified loop invariants & explained complexity |\n",
    "| **high (10 000)**| 10 000           | **+ 0.1765** | produced full design doc, edge-case tests & refactor suggestions |\n",
    "\n",
    "**Insights**\n",
    "\n",
    "* The *first* ~2 k reasoning tokens captured ≈ 90 % of useful explanation.  \n",
    "* Jumping from **low** → **high** added 7 952 tokens and ≈ \\$0.11 per call.  \n",
    "* Unless you need an audit trail or pedagogical commentary, the *low*\n",
    "  preset is the sweet-spot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ 3 · Choosing the Right Model & Reasoning Budget (2025)\n",
    "\n",
    "1. **Rapid prototyping / IDE autocomplete** → **Haiku 3.5**\n",
    "   Lowest latency keeps the feedback loop tight.\n",
    "\n",
    "2. **Mainline coding tasks** → **Sonnet 3.7 (low)**\n",
    "   Better reasoning than Haiku, modest cost, transparent chain-of-thought.\n",
    "\n",
    "3. **Hard algorithms or large context** → **Sonnet 3.7 (high)**\n",
    "   Use when you need > 8 k context or a fully detailed rationale.\n",
    "\n",
    "4. **Compliance / Audit**\n",
    "   Persist the entire reasoning text plus `thinking_tokens` for audits.\n",
    "\n",
    "5. **Opus**\n",
    "   Enable only when you truly need the very best model and cost is secondary.\n",
    "\n",
    "6. **Budget tip**\n",
    "   Cost grows linearly with reasoning tokens.\n",
    "   Trimming the chain from 10 k → 4 k saves ≈ 6 k · \\$0.003 = \\$0.018 per call.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ▶️ Next Step — Running Your Own Benchmarks\n",
    "\n",
    "Feel free to adapt and extend these benchmarks with other models available to you. Popular alternatives include:\n",
    "\n",
    "Google Vertex AI (Gemini models)\n",
    "\n",
    "OpenAI GPT models\n",
    "\n",
    "Meta's LLaMA family\n",
    "\n",
    "Locally-hosted open-source models\n",
    "\n",
    "Comparing multiple models allows a more comprehensive understanding of each model's strengths and weaknesses, ensuring optimal selection based on your specific use case and constraints.\n",
    "\n",
    "### We’ll now switch clouds and run the **same questions** against **Gemini Pro 2.5** (free quota on Vertex AI).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
